{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2017/18\n",
    "\n",
    "### Practice 4 - Similarity with Cython\n",
    "\n",
    "\n",
    "### Cython is a superset of Python, allowing you to use C-like operations and import C code. Cython files (.pyx) are compiled and support static typing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's implement something simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isPrime(n):\n",
    "    \n",
    "    i = 2\n",
    "    \n",
    "    # Usually you loop up to sqrt(n)\n",
    "    while i < n:\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is prime 2? True\n",
      "Is prime 3? True\n",
      "Is prime 5? True\n",
      "Is prime 15? False\n",
      "Is prime 20? False\n"
     ]
    }
   ],
   "source": [
    "print(\"Is prime 2? {}\".format(isPrime(2)))\n",
    "print(\"Is prime 3? {}\".format(isPrime(3)))\n",
    "print(\"Is prime 5? {}\".format(isPrime(5)))\n",
    "print(\"Is prime 15? {}\".format(isPrime(15)))\n",
    "print(\"Is prime 20? {}\".format(isPrime(20)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Prime 50000017? True, time required 4.71 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "result = isPrime(50000017)\n",
    "\n",
    "print(\"Is Prime 50000017? {}, time required {:.2f} sec\".format(result, time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Cython magic command, this takes care of the compilation step. If you are writing code outside Jupyter you'll have to compile using other tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Declare Cython function, paste the same code as before. The function will be compiled and then executed with a Python interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "def isPrime(n):\n",
    "    \n",
    "    i = 2\n",
    "    \n",
    "    # Usually you loop up to sqrt(n)\n",
    "    while i < n:\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Prime 50000017? True, time required 2.45 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "result = isPrime(50000017)\n",
    "\n",
    "print(\"Is Prime 50000017? {}, time required {:.2f} sec\".format(result, time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### As you can see by just compiling the same code we got some improvement.\n",
    "#### To go seriously higher, we have to use some static tiping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "# Declare the tipe of the arguments\n",
    "def isPrime(long n):\n",
    "    \n",
    "    # Declare index of for loop\n",
    "    cdef long i\n",
    "    \n",
    "    i = 2\n",
    "    \n",
    "    # Usually you loop up to sqrt(n)\n",
    "    while i < n:\n",
    "        if n % i == 0:\n",
    "            return False\n",
    "        \n",
    "        i += 1\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is Prime 50000017? True, time required 0.43 sec\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "result = isPrime(50000017)\n",
    "\n",
    "print(\"Is Prime 50000017? {}, time required {:.2f} sec\".format(result, time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cython code with two tipe declaration, for n and i, runs 50x faster than Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main benefits of Cython:\n",
    "* Compiled, no interpreter\n",
    "* Static typing, no overhead\n",
    "* Fast loops, no need to vectorize. Vectorization sometimes performes lots of useless operations\n",
    "* Numpy, which is fast in python, becomes often slooooow compared to a carefully written Cython code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity with Cython\n",
    "\n",
    "#### Load the usual data. I created a reader to encapsulate the data-specific format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movielens10MReader: loading data...\n",
      "Processed 1000000 cells\n",
      "Processed 2000000 cells\n",
      "Processed 3000000 cells\n",
      "Processed 4000000 cells\n",
      "Processed 5000000 cells\n",
      "Processed 6000000 cells\n",
      "Processed 7000000 cells\n",
      "Processed 8000000 cells\n",
      "Processed 1000000 cells\n"
     ]
    }
   ],
   "source": [
    "from Movielens10MReader import Movielens10MReader\n",
    "\n",
    "dataReader = Movielens10MReader()\n",
    "\n",
    "URM_train = dataReader.get_URM_train()\n",
    "URM_test = dataReader.get_URM_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<71568x65134 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 8000085 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Since we cannot store in memory the whole similarity, we compute it one row at a time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(71568,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemIndex=1\n",
    "item_ratings = URM_train[:,itemIndex]\n",
    "item_ratings = item_ratings.toarray().squeeze()\n",
    "\n",
    "item_ratings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(65134,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "this_item_weights = URM_train.T.dot(item_ratings)\n",
    "this_item_weights.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Once we have the scores for that row, we get the TopK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1196,  110,  480, 1210,  593,  318,  356,  296,  260,    1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "k=10\n",
    "\n",
    "top_k_idx = np.argsort(this_item_weights) [-k:]\n",
    "top_k_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function hiding some conversion checks\n",
    "def check_matrix(X, format='csc', dtype=np.float32):\n",
    "    if format == 'csc' and not isinstance(X, sps.csc_matrix):\n",
    "        return X.tocsc().astype(dtype)\n",
    "    elif format == 'csr' and not isinstance(X, sps.csr_matrix):\n",
    "        return X.tocsr().astype(dtype)\n",
    "    elif format == 'coo' and not isinstance(X, sps.coo_matrix):\n",
    "        return X.tocoo().astype(dtype)\n",
    "    elif format == 'dok' and not isinstance(X, sps.dok_matrix):\n",
    "        return X.todok().astype(dtype)\n",
    "    elif format == 'bsr' and not isinstance(X, sps.bsr_matrix):\n",
    "        return X.tobsr().astype(dtype)\n",
    "    elif format == 'dia' and not isinstance(X, sps.dia_matrix):\n",
    "        return X.todia().astype(dtype)\n",
    "    elif format == 'lil' and not isinstance(X, sps.lil_matrix):\n",
    "        return X.tolil().astype(dtype)\n",
    "    else:\n",
    "        return X.astype(dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Basic Collaborative filtering recommender using only cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class BasicItemKNN_CF_Recommender(object):\n",
    "    \"\"\" ItemKNN recommender with cosine similarity and no shrinkage\"\"\"\n",
    "\n",
    "    def __init__(self, URM, k=50, shrinkage=100):\n",
    "        self.dataset = URM\n",
    "        self.k = k\n",
    "        self.shrinkage = shrinkage\n",
    "        \n",
    "    def __str__(self):\n",
    "        return \"ItemKNN(similarity={},k={},shrinkage={})\".format(\n",
    "            'cosine', self.k, self.shrinkage)\n",
    "    \n",
    "    def compute_similarity(self, URM):\n",
    "        \n",
    "        # We explore the matrix column-wise\n",
    "        URM = check_matrix(URM, 'csc')     \n",
    "                    \n",
    "        values = []\n",
    "        rows = []\n",
    "        cols = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        processedItems = 0\n",
    "        \n",
    "        # Compute all similarities for each item using vectorization\n",
    "        for itemIndex in range(URM.shape[0]):\n",
    "            \n",
    "            processedItems += 1\n",
    "            \n",
    "            if processedItems % 100==0:\n",
    "                \n",
    "                itemPerSec = processedItems/(time.time()-start_time)\n",
    "                \n",
    "                print(\"Similarity item {}, {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                    processedItems, itemPerSec, URM.shape[0]/itemPerSec/60))\n",
    "            \n",
    "            # All ratings for a given item\n",
    "            item_ratings = URM[:,itemIndex]\n",
    "            item_ratings = item_ratings.toarray().squeeze()\n",
    "            \n",
    "            # Compute item similarities\n",
    "            this_item_weights = URM_train.T.dot(item_ratings)\n",
    "            \n",
    "            # Sort indices and select TopK\n",
    "            top_k_idx = np.argsort(this_item_weights) [-self.k:]\n",
    "            \n",
    "            # Incrementally build sparse matrix\n",
    "            values.extend(this_item_weights[top_k_idx])\n",
    "            rows.extend(np.arange(URM.shape[0])[top_k_idx])\n",
    "            cols.extend(np.ones(self.k) * itemIndex)\n",
    "            \n",
    "        self.W_sparse = sps.csc_matrix((values, (rows, cols)),\n",
    "                                       shape=(URM.shape[0], URM.shape[0]),\n",
    "                                       dtype=np.float32)\n",
    "\n",
    "        \n",
    "\n",
    "    def fit(self):\n",
    "        item_weights = self.compute_similarity(self.dataset)\n",
    "        \n",
    "        item_weights = check_matrix(item_weights, 'csr')\n",
    "        \n",
    "        \n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        \n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.dataset[user_id]\n",
    "        scores = user_profile.dot(self.W_sparse).toarray().ravel()\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1]\n",
    "        if exclude_seen:\n",
    "            ranking = self._filter_seen(user_id, ranking)\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    def _filter_seen(self, user_id, ranking):\n",
    "        user_profile = self.dataset[user_id]\n",
    "        seen = user_profile.indices\n",
    "        unseen_mask = np.in1d(ranking, seen, assume_unique=True, invert=True)\n",
    "        return ranking[unseen_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's isolate the compute_similarity function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_similarity(URM, k=100):\n",
    "\n",
    "    # We explore the matrix column-wise\n",
    "    URM = check_matrix(URM, 'csc')\n",
    "    \n",
    "    n_items = URM.shape[0]\n",
    "\n",
    "    values = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    processedItems = 0\n",
    "\n",
    "    # Compute all similarities for each item using vectorization\n",
    "    # for itemIndex in range(n_items):\n",
    "    for itemIndex in range(1000):\n",
    "\n",
    "        processedItems += 1\n",
    "\n",
    "        if processedItems % 100==0:\n",
    "\n",
    "            itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "            print(\"Similarity item {}, {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                processedItems, itemPerSec, n_items/itemPerSec/60))\n",
    "\n",
    "        # All ratings for a given item\n",
    "        item_ratings = URM[:,itemIndex]\n",
    "        item_ratings = item_ratings.toarray().squeeze()\n",
    "\n",
    "        # Compute item similarities\n",
    "        this_item_weights = URM.T.dot(item_ratings)\n",
    "\n",
    "        # Sort indices and select TopK\n",
    "        top_k_idx = np.argsort(this_item_weights) [-k:]\n",
    "\n",
    "        # Incrementally build sparse matrix\n",
    "        values.extend(this_item_weights[top_k_idx])\n",
    "        rows.extend(np.arange(URM.shape[0])[top_k_idx])\n",
    "        cols.extend(np.ones(k) * itemIndex)\n",
    "\n",
    "    W_sparse = sps.csc_matrix((values, (rows, cols)),\n",
    "                            shape=(n_items, n_items),\n",
    "                            dtype=np.float32)\n",
    "\n",
    "    return W_sparse\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity item 100, 78.08 item/sec, required time 15.28 min\n",
      "Similarity item 200, 77.20 item/sec, required time 15.45 min\n",
      "Similarity item 300, 76.19 item/sec, required time 15.66 min\n",
      "Similarity item 400, 77.18 item/sec, required time 15.45 min\n",
      "Similarity item 500, 77.51 item/sec, required time 15.39 min\n",
      "Similarity item 600, 77.61 item/sec, required time 15.37 min\n",
      "Similarity item 700, 77.83 item/sec, required time 15.33 min\n",
      "Similarity item 800, 77.70 item/sec, required time 15.35 min\n",
      "Similarity item 900, 77.75 item/sec, required time 15.34 min\n",
      "Similarity item 1000, 77.48 item/sec, required time 15.39 min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<71568x71568 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 100000 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_similarity(URM_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### We see that computing the similarity takes more or less 15 minutes\n",
    "### Now we use the same identical code, but we compile it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "def compute_similarity_compiled(URM, k=100):\n",
    "\n",
    "    # We explore the matrix column-wise\n",
    "    URM = URM.tocsc()\n",
    "    \n",
    "    n_items = URM.shape[0]\n",
    "\n",
    "    values = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    processedItems = 0\n",
    "\n",
    "    # Compute all similarities for each item using vectorization\n",
    "    # for itemIndex in range(n_items):\n",
    "    for itemIndex in range(1000):\n",
    "\n",
    "        processedItems += 1\n",
    "\n",
    "        if processedItems % 100==0:\n",
    "\n",
    "            itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "            print(\"Similarity item {}, {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                processedItems, itemPerSec, n_items/itemPerSec/60))\n",
    "\n",
    "        # All ratings for a given item\n",
    "        item_ratings = URM[:,itemIndex]\n",
    "        item_ratings = item_ratings.toarray().squeeze()\n",
    "\n",
    "        # Compute item similarities\n",
    "        this_item_weights = URM.T.dot(item_ratings)\n",
    "\n",
    "        # Sort indices and select TopK\n",
    "        top_k_idx = np.argsort(this_item_weights) [-k:]\n",
    "\n",
    "        # Incrementally build sparse matrix\n",
    "        values.extend(this_item_weights[top_k_idx])\n",
    "        rows.extend(np.arange(URM.shape[0])[top_k_idx])\n",
    "        cols.extend(np.ones(k) * itemIndex)\n",
    "\n",
    "    W_sparse = sps.csc_matrix((values, (rows, cols)),\n",
    "                            shape=(n_items, n_items),\n",
    "                            dtype=np.float32)\n",
    "\n",
    "    return W_sparse\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity item 100, 79.84 item/sec, required time 14.94 min\n",
      "Similarity item 200, 80.12 item/sec, required time 14.89 min\n",
      "Similarity item 300, 79.39 item/sec, required time 15.03 min\n",
      "Similarity item 400, 79.29 item/sec, required time 15.04 min\n",
      "Similarity item 500, 79.43 item/sec, required time 15.02 min\n",
      "Similarity item 600, 79.60 item/sec, required time 14.99 min\n",
      "Similarity item 700, 79.87 item/sec, required time 14.93 min\n",
      "Similarity item 800, 80.11 item/sec, required time 14.89 min\n",
      "Similarity item 900, 80.17 item/sec, required time 14.88 min\n",
      "Similarity item 1000, 80.14 item/sec, required time 14.88 min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<71568x71568 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 100000 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_similarity_compiled(URM_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### As opposed to the previous example, compilation by itself is not very helpful. Why?\n",
    "#### Because the compiler is just porting in C all operations that the python interpreter would have to perform, dynamic tiping included\n",
    "\n",
    "### Now try to add some tipes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import time\n",
    "import numpy as np\n",
    "import scipy.sparse as sps\n",
    "\n",
    "cimport numpy as np\n",
    "\n",
    "def compute_similarity_compiled(URM, int k=100):\n",
    "    \n",
    "    cdef int itemIndex, processedItems\n",
    "    \n",
    "    # We use the numpy syntax, allowing us to perform vectorized operations\n",
    "    cdef np.ndarray[float, ndim=1] item_ratings, this_item_weights\n",
    "    cdef np.ndarray[long, ndim=1] top_k_idx\n",
    "\n",
    "    # We explore the matrix column-wise\n",
    "    URM = URM.tocsc()\n",
    "    \n",
    "    n_items = URM.shape[0]\n",
    "\n",
    "    values = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    processedItems = 0\n",
    "\n",
    "    # Compute all similarities for each item using vectorization\n",
    "    # for itemIndex in range(n_items):\n",
    "    for itemIndex in range(1000):\n",
    "\n",
    "        processedItems += 1\n",
    "\n",
    "        if processedItems % 100==0:\n",
    "\n",
    "            itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "            print(\"Similarity item {}, {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                processedItems, itemPerSec, n_items/itemPerSec/60))\n",
    "\n",
    "        # All ratings for a given item\n",
    "        item_ratings = URM[:,itemIndex].toarray().squeeze()\n",
    "\n",
    "        # Compute item similarities\n",
    "        this_item_weights = URM.T.dot(item_ratings)\n",
    "\n",
    "        # Sort indices and select TopK\n",
    "        top_k_idx = np.argsort(this_item_weights) [-k:]\n",
    "\n",
    "        # Incrementally build sparse matrix\n",
    "        values.extend(this_item_weights[top_k_idx])\n",
    "        rows.extend(np.arange(URM.shape[0])[top_k_idx])\n",
    "        cols.extend(np.ones(k) * itemIndex)\n",
    "\n",
    "    W_sparse = sps.csc_matrix((values, (rows, cols)),\n",
    "                            shape=(n_items, n_items),\n",
    "                            dtype=np.float32)\n",
    "\n",
    "    return W_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity item 100, 82.45 item/sec, required time 14.47 min\n",
      "Similarity item 200, 81.82 item/sec, required time 14.58 min\n",
      "Similarity item 300, 81.51 item/sec, required time 14.63 min\n",
      "Similarity item 400, 81.14 item/sec, required time 14.70 min\n",
      "Similarity item 500, 81.16 item/sec, required time 14.70 min\n",
      "Similarity item 600, 81.28 item/sec, required time 14.68 min\n",
      "Similarity item 700, 80.79 item/sec, required time 14.76 min\n",
      "Similarity item 800, 79.44 item/sec, required time 15.02 min\n",
      "Similarity item 900, 78.56 item/sec, required time 15.18 min\n",
      "Similarity item 1000, 78.69 item/sec, required time 15.16 min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<71568x71568 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 100000 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_similarity_compiled(URM_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Still no luck! Why?\n",
    "### There are a few reasons:\n",
    "* We are getting the data from the sparse matrix using its interface, which is SLOW\n",
    "* We are transforming sparse data into a dense array, which is SLOW\n",
    "* We are performing a dot product against a dense vector\n",
    "\n",
    "#### You colud find a workaround... here we do something different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proposed solution\n",
    "### Change the algorithm!\n",
    "\n",
    "### Instead of performing the dot product, let's implement somenting that computes the similarity using directly sparse data\n",
    "\n",
    "### We loop through the data and update selectively the similarity matrix cells. \n",
    "### Underlying idea:\n",
    "* When I select an item I can know which users rated it\n",
    "* Instead of looping through the other items trying to find common users, I use the URM to find which other items that user rated\n",
    "* The user I am considering will be common between the two, so I increment the similarity of the two items\n",
    "* Instead of following the path item1 -> loop item2 -> find user, i go item1 -> loop user -> loop item2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[1, 1, 0, 1],\n",
       "        [0, 1, 1, 1],\n",
       "        [1, 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_matrix = np.array([[1,1,0,1],[0,1,1,1],[1,0,1,0]])\n",
    "data_matrix = sps.csc_matrix(data_matrix)\n",
    "data_matrix.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Compute the similarities for item 1\n",
    "\n",
    "#### Step 1: get users that rated item 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int32)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_rated_item = data_matrix[:,1]\n",
    "users_rated_item.indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: count how many times those users rated other items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 1, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "item_similarity = data_matrix[users_rated_item.indices].sum(axis = 0)\n",
    "np.array(item_similarity).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Verify our result against the common method. We can see that the similarity values for col 1 are identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 1, 1],\n",
       "       [1, 2, 1, 2],\n",
       "       [1, 1, 2, 1],\n",
       "       [1, 2, 1, 2]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "similarity_matrix_product = data_matrix.T.dot(data_matrix)\n",
    "similarity_matrix_product.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The following code works for implicit feedback only\n",
    "def compute_similarity_new_algorithm(URM, k=100):\n",
    "\n",
    "    # We explore the matrix column-wise\n",
    "    URM = check_matrix(URM, 'csc')\n",
    "    URM.data = np.ones_like(URM.data)\n",
    "    \n",
    "    n_items = URM.shape[0]\n",
    "\n",
    "    values = []\n",
    "    rows = []\n",
    "    cols = []\n",
    "\n",
    "    start_time = time.time()\n",
    "    processedItems = 0\n",
    "\n",
    "    # Compute all similarities for each item using vectorization\n",
    "    # for itemIndex in range(n_items):\n",
    "    for itemIndex in range(1000):\n",
    "\n",
    "        processedItems += 1\n",
    "\n",
    "        if processedItems % 100==0:\n",
    "\n",
    "            itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "            print(\"Similarity item {}, {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                processedItems, itemPerSec, n_items/itemPerSec/60))\n",
    "\n",
    "        # All ratings for a given item\n",
    "        users_rated_item = URM.indices[URM.indptr[itemIndex]:URM.indptr[itemIndex+1]]\n",
    "\n",
    "        # Compute item similarities\n",
    "        this_item_weights = URM[users_rated_item].sum(axis = 0)\n",
    "        this_item_weights = np.array(this_item_weights).squeeze()\n",
    "\n",
    "        # Sort indices and select TopK\n",
    "        top_k_idx = np.argsort(this_item_weights) [-k:]\n",
    "\n",
    "        # Incrementally build sparse matrix\n",
    "        values.extend(this_item_weights[top_k_idx])\n",
    "        rows.extend(np.arange(URM.shape[0])[top_k_idx])\n",
    "        cols.extend(np.ones(k) * itemIndex)\n",
    "\n",
    "    W_sparse = sps.csc_matrix((values, (rows, cols)),\n",
    "                            shape=(n_items, n_items),\n",
    "                            dtype=np.float32)\n",
    "\n",
    "    return W_sparse\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity item 100, 22.65 item/sec, required time 52.67 min\n",
      "Similarity item 200, 23.71 item/sec, required time 50.32 min\n",
      "Similarity item 300, 24.07 item/sec, required time 49.55 min\n",
      "Similarity item 400, 23.50 item/sec, required time 50.75 min\n",
      "Similarity item 500, 23.90 item/sec, required time 49.90 min\n",
      "Similarity item 600, 23.43 item/sec, required time 50.90 min\n",
      "Similarity item 700, 24.32 item/sec, required time 49.04 min\n",
      "Similarity item 800, 24.73 item/sec, required time 48.24 min\n",
      "Similarity item 900, 25.42 item/sec, required time 46.92 min\n",
      "Similarity item 1000, 25.53 item/sec, required time 46.73 min\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<71568x71568 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 100000 stored elements in Compressed Sparse Column format>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_similarity_new_algorithm(URM_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Slower but expected, dot product operations are implemented in an efficient way and here we are using an indirect approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's write this algorithm in Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "from cpython.array cimport array, clone\n",
    "\n",
    "import scipy.sparse as sps\n",
    "\n",
    "\n",
    "cdef class Cosine_Similarity:\n",
    "\n",
    "    cdef int TopK\n",
    "    cdef long n_items\n",
    "\n",
    "    # Arrays containing the sparse data\n",
    "    cdef int[:] user_to_item_row_ptr, user_to_item_cols\n",
    "    cdef int[:] item_to_user_rows, item_to_user_col_ptr\n",
    "    cdef double[:] user_to_item_data, item_to_user_data\n",
    "\n",
    "    # In case you select no TopK\n",
    "    cdef double[:,:] W_dense\n",
    "\n",
    "    \n",
    "    def __init__(self, URM, TopK = 100):\n",
    "        \"\"\"\n",
    "        Dataset must be a matrix with items as columns\n",
    "        :param dataset:\n",
    "        :param TopK:\n",
    "        \"\"\"\n",
    "\n",
    "        super(Cosine_Similarity, self).__init__()\n",
    "\n",
    "        self.n_items = URM.shape[1]\n",
    "\n",
    "        self.TopK = min(TopK, self.n_items)\n",
    "\n",
    "        URM = URM.tocsr()\n",
    "        self.user_to_item_row_ptr = URM.indptr\n",
    "        self.user_to_item_cols = URM.indices\n",
    "        self.user_to_item_data = np.array(URM.data, dtype=np.float64)\n",
    "\n",
    "        URM = URM.tocsc()\n",
    "        self.item_to_user_rows = URM.indices\n",
    "        self.item_to_user_col_ptr = URM.indptr\n",
    "        self.item_to_user_data = np.array(URM.data, dtype=np.float64)\n",
    "\n",
    "        if self.TopK == 0:\n",
    "            self.W_dense = np.zeros((self.n_items,self.n_items))\n",
    "\n",
    "\n",
    "\n",
    "    cdef int[:] getUsersThatRatedItem(self, long item_id):\n",
    "        return self.item_to_user_rows[self.item_to_user_col_ptr[item_id]:self.item_to_user_col_ptr[item_id+1]]\n",
    "\n",
    "    cdef int[:] getItemsRatedByUser(self, long user_id):\n",
    "        return self.user_to_item_cols[self.user_to_item_row_ptr[user_id]:self.user_to_item_row_ptr[user_id+1]]\n",
    "\n",
    "    \n",
    "    \n",
    "    cdef double[:] computeItemSimilarities(self, long item_id_input):\n",
    "        \"\"\"\n",
    "        For every item the cosine similarity against other items depends on whether they have users in common. \n",
    "        The more common users the higher the similarity.\n",
    "        \n",
    "        The basic implementation is:\n",
    "        - Select the first item\n",
    "        - Loop through all other items\n",
    "        -- Given the two items, get the users they have in common\n",
    "        -- Update the similarity considering all common users\n",
    "        \n",
    "        That is VERY slow due to the common user part, in which a long data structure is looped multiple times.\n",
    "        \n",
    "        A better way is to use the data structure in a different way skipping the search part, getting directly\n",
    "        the information we need.\n",
    "        \n",
    "        The implementation here used is:\n",
    "        - Select the first item\n",
    "        - Initialize a zero valued array for the similarities\n",
    "        - Get the users who rated the first item\n",
    "        - Loop through the users\n",
    "        -- Given a user, get the items he rated (second item)\n",
    "        -- Update the similarity of the items he rated\n",
    "        \n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "        # Create template used to initialize an array with zeros\n",
    "        # Much faster than np.zeros(self.n_items)\n",
    "        cdef array[double] template_zero = array('d')\n",
    "        cdef array[double] result = clone(template_zero, self.n_items, zero=True)\n",
    "\n",
    "\n",
    "        cdef long user_index, user_id, item_index, item_id_second\n",
    "\n",
    "        cdef int[:] users_that_rated_item = self.getUsersThatRatedItem(item_id_input)\n",
    "        cdef int[:] items_rated_by_user\n",
    "\n",
    "        cdef double rating_item_input, rating_item_second\n",
    "\n",
    "        # Get users that rated the items\n",
    "        for user_index in range(len(users_that_rated_item)):\n",
    "\n",
    "            user_id = users_that_rated_item[user_index]\n",
    "            rating_item_input = self.item_to_user_data[self.item_to_user_col_ptr[item_id_input]+user_index]\n",
    "\n",
    "            # Get all items rated by that user\n",
    "            items_rated_by_user = self.getItemsRatedByUser(user_id)\n",
    "\n",
    "            for item_index in range(len(items_rated_by_user)):\n",
    "\n",
    "                item_id_second = items_rated_by_user[item_index]\n",
    "\n",
    "                # Do not compute the similarity on the diagonal\n",
    "                if item_id_second != item_id_input:\n",
    "                    # Increment similairty\n",
    "                    rating_item_second = self.user_to_item_data[self.user_to_item_row_ptr[user_id]+item_index]\n",
    "\n",
    "                    result[item_id_second] += rating_item_input*rating_item_second\n",
    "\n",
    "        return result\n",
    "\n",
    "\n",
    "    def compute_similarity(self):\n",
    "\n",
    "        cdef int itemIndex, innerItemIndex\n",
    "        cdef long long topKItemIndex\n",
    "\n",
    "        cdef long long[:] top_k_idx\n",
    "\n",
    "        # Declare numpy data type to use vetor indexing and simplify the topK selection code\n",
    "        cdef np.ndarray[long, ndim=1] top_k_partition, top_k_partition_sorting\n",
    "        cdef np.ndarray[np.float64_t, ndim=1] this_item_weights_np\n",
    "\n",
    "        #cdef long[:] top_k_idx\n",
    "        cdef double[:] this_item_weights\n",
    "\n",
    "        cdef long processedItems = 0\n",
    "\n",
    "        # Data structure to incrementally build sparse matrix\n",
    "        # Preinitialize max possible length\n",
    "        cdef double[:] values = np.zeros((self.n_items*self.TopK))\n",
    "        cdef int[:] rows = np.zeros((self.n_items*self.TopK,), dtype=np.int32)\n",
    "        cdef int[:] cols = np.zeros((self.n_items*self.TopK,), dtype=np.int32)\n",
    "        cdef long sparse_data_pointer = 0\n",
    "\n",
    "\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Compute all similarities for each item\n",
    "        for itemIndex in range(self.n_items):\n",
    "\n",
    "            processedItems += 1\n",
    "\n",
    "            if processedItems % 10000==0 or processedItems==self.n_items:\n",
    "\n",
    "                itemPerSec = processedItems/(time.time()-start_time)\n",
    "\n",
    "                print(\"Similarity item {} ( {:2.0f} % ), {:.2f} item/sec, required time {:.2f} min\".format(\n",
    "                    processedItems, processedItems*1.0/self.n_items*100, itemPerSec, (self.n_items-processedItems) / itemPerSec / 60))\n",
    "\n",
    "            this_item_weights = self.computeItemSimilarities(itemIndex)\n",
    "\n",
    "            if self.TopK == 0:\n",
    "\n",
    "                for innerItemIndex in range(self.n_items):\n",
    "                    self.W_dense[innerItemIndex,itemIndex] = this_item_weights[innerItemIndex]\n",
    "\n",
    "            else:\n",
    "\n",
    "                # Sort indices and select TopK\n",
    "                # Using numpy implies some overhead, unfortunately the plain C qsort function is even slower\n",
    "                # top_k_idx = np.argsort(this_item_weights) [-self.TopK:]\n",
    "\n",
    "                # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "                # because we avoid sorting elements we already know we don't care about\n",
    "                # - Partition the data to extract the set of TopK items, this set is unsorted\n",
    "                # - Sort only the TopK items, discarding the rest\n",
    "                # - Get the original item index\n",
    "\n",
    "                this_item_weights_np = - np.array(this_item_weights)\n",
    "                \n",
    "                # Get the unordered set of topK items\n",
    "                top_k_partition = np.argpartition(this_item_weights_np, self.TopK-1)[0:self.TopK]\n",
    "                # Sort only the elements in the partition\n",
    "                top_k_partition_sorting = np.argsort(this_item_weights_np[top_k_partition])\n",
    "                # Get original index\n",
    "                top_k_idx = top_k_partition[top_k_partition_sorting]\n",
    "\n",
    "\n",
    "\n",
    "                # Incrementally build sparse matrix\n",
    "                for innerItemIndex in range(len(top_k_idx)):\n",
    "\n",
    "                    topKItemIndex = top_k_idx[innerItemIndex]\n",
    "\n",
    "                    values[sparse_data_pointer] = this_item_weights[topKItemIndex]\n",
    "                    rows[sparse_data_pointer] = topKItemIndex\n",
    "                    cols[sparse_data_pointer] = itemIndex\n",
    "\n",
    "                    sparse_data_pointer += 1\n",
    "\n",
    "\n",
    "        if self.TopK == 0:\n",
    "\n",
    "            return np.array(self.W_dense)\n",
    "\n",
    "        else:\n",
    "\n",
    "            values = np.array(values[0:sparse_data_pointer])\n",
    "            rows = np.array(rows[0:sparse_data_pointer])\n",
    "            cols = np.array(cols[0:sparse_data_pointer])\n",
    "\n",
    "            W_sparse = sps.csr_matrix((values, (rows, cols)),\n",
    "                                    shape=(self.n_items, self.n_items),\n",
    "                                    dtype=np.float32)\n",
    "\n",
    "            return W_sparse\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity item 10000 ( 15 % ), 542.42 item/sec, required time 1.69 min\n",
      "Similarity item 20000 ( 31 % ), 956.34 item/sec, required time 0.79 min\n",
      "Similarity item 30000 ( 46 % ), 1276.94 item/sec, required time 0.46 min\n",
      "Similarity item 40000 ( 61 % ), 1525.23 item/sec, required time 0.27 min\n",
      "Similarity item 50000 ( 77 % ), 1726.64 item/sec, required time 0.15 min\n",
      "Similarity item 60000 ( 92 % ), 1884.95 item/sec, required time 0.05 min\n",
      "Similarity item 65134 ( 100 % ), 1968.23 item/sec, required time 0.00 min\n",
      "Similarity computed in 33.89 seconds\n"
     ]
    }
   ],
   "source": [
    "cosine_cython = Cosine_Similarity(URM_train, TopK=100)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "cosine_cython.compute_similarity()\n",
    "\n",
    "print(\"Similarity computed in {:.2f} seconds\".format(time.time()-start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Better... much better. There are a few other things you could do, but at this point it is not worth the effort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
