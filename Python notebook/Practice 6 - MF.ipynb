{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2017/18\n",
    "\n",
    "### Practice session on BPR-MF\n",
    "\n",
    "\n",
    "## Recap on BPR\n",
    "S.Rendle et al. BPR: Bayesian Personalized Ranking from Implicit Feedback. UAI2009\n",
    "\n",
    "The usual approach for item recommenders is to predict a personalized score $\\hat{x}_{ui}$ for an item that reflects the preference of the user for the item. Then the items are ranked by sorting them according to that score.\n",
    "\n",
    "Machine learning approaches are tipically fit by using observed items as a positive sample and missing ones for the negative class. A perfect model would thus be useless, as it would classify as negative (non-interesting) all the items that were non-observed at training time. The only reason why such methods work is regularization.\n",
    "\n",
    "BPR use a different approach. The training dataset is composed by triplets $(u,i,j)$ representing that user u is assumed to prefer i over j. For an implicit dataset this means that u observed i but not j:\n",
    "$$D_S := \\{(u,i,j) \\mid i \\in I_u^+ \\wedge j \\in I \\setminus I_u^+\\}$$\n",
    "\n",
    "### BPR-OPT\n",
    "A machine learning model can be represented by a parameter vector $\\Theta$ which is found at fitting time. BPR wants to find the parameter vector that is most probable given the desired, but latent, preference structure $>_u$:\n",
    "$$p(\\Theta \\mid >_u) \\propto p(>_u \\mid \\Theta)p(\\Theta) $$\n",
    "$$\\prod_{u\\in U} p(>_u \\mid \\Theta) = \\dots = \\prod_{(u,i,j) \\in D_S} p(i >_u j \\mid \\Theta) $$\n",
    "\n",
    "The probability that a user really prefers item $i$ to item $j$ is defined as:\n",
    "$$ p(i >_u j \\mid \\Theta) := \\sigma(\\hat{x}_{uij}(\\Theta)) $$\n",
    "Where $\\sigma$ represent the logistic sigmoid and $\\hat{x}_{uij}(\\Theta)$ is an arbitrary real-valued function of $\\Theta$ (the output of your arbitrary model).\n",
    "\n",
    "\n",
    "To complete the Bayesian setting, we define a prior density for the parameters:\n",
    "$$p(\\Theta) \\sim N(0, \\Sigma_\\Theta)$$\n",
    "And we can now formulate the maximum posterior estimator:\n",
    "$$BPR-OPT := \\log p(\\Theta \\mid >_u) $$\n",
    "$$ = \\log p(>_u \\mid \\Theta) p(\\Theta) $$\n",
    "$$ = \\log \\prod_{(u,i,j) \\in D_S} \\sigma(\\hat{x}_{uij})p(\\Theta) $$\n",
    "$$ = \\sum_{(u,i,j) \\in D_S} \\log \\sigma(\\hat{x}_{uij}) + \\log p(\\Theta) $$\n",
    "$$ = \\sum_{(u,i,j) \\in D_S} \\log \\sigma(\\hat{x}_{uij}) - \\lambda_\\Theta ||\\Theta||^2 $$\n",
    "\n",
    "Where $\\lambda_\\Theta$ are model specific regularization parameters.\n",
    "\n",
    "### BPR learning algorithm\n",
    "Once obtained the log-likelihood, we need to maximize it in order to find our obtimal $\\Theta$. As the crierion is differentiable, gradient descent algorithms are an obvious choiche for maximization.\n",
    "\n",
    "Gradient descent comes in many fashions, you can find an overview on my master thesis https://www.politesi.polimi.it/bitstream/10589/133864/3/tesi.pdf on pages 18-19-20 (I'm linking my thesis just because I'm sure of what it's written there, many posts you can find online contain some error). A nice post about momentum is available here https://distill.pub/2017/momentum/\n",
    "\n",
    "The basic version of gradient descent consists in evaluating the gradient using all the available samples and then perform a single update. The problem with this is, in our case, that our training dataset is very skewed. Suppose an item i is very popular. Then we habe many terms of the form $\\hat{x}_{uij}$ in the loss because for many users u the item i is compared against all negative items j.\n",
    "\n",
    "The other popular approach is stochastic gradient descent, where for each training sample an update is performed. This is a better approach, but the order in which the samples are traversed is crucial. To solve this issue BPR uses a stochastic gradient descent algorithm that choses the triples randomly.\n",
    "\n",
    "The gradient of BPR-OPT with respect to the model parameters is: \n",
    "$$\\frac{\\partial BPR-OPT}{\\partial \\Theta} = \\sum_{(u,i,j) \\in D_S} \\frac{\\partial}{\\partial \\Theta} \\log \\sigma (\\hat{x}_{uij}) - \\lambda_\\Theta \\frac{\\partial}{\\partial\\Theta} || \\Theta ||^2$$\n",
    "$$ =  \\sum_{(u,i,j) \\in D_S} \\frac{-e^{-\\hat{x}_{uij}}}{1+e^{-\\hat{x}_{uij}}} \\frac{\\partial}{\\partial \\Theta}\\hat{x}_{uij} - \\lambda_\\Theta \\Theta $$\n",
    "\n",
    "### BPR-MF\n",
    "\n",
    "In order to practically apply this learning schema to an existing algorithm, we first split the real valued preference term: $\\hat{x}_{uij} := \\hat{x}_{ui} − \\hat{x}_{uj}$. And now we can apply any standard collaborative filtering model that predicts $\\hat{x}_{ui}$.\n",
    "\n",
    "The problem of predicting $\\hat{x}_{ui}$ can be seen as the task of estimating a matrix $X:U×I$. With matrix factorization teh target matrix $X$ is approximated by the matrix product of two low-rank matrices $W:|U|\\times k$ and $H:|I|\\times k$:\n",
    "$$X := WH^t$$\n",
    "The prediction formula can also be written as:\n",
    "$$\\hat{x}_{ui} = \\langle w_u,h_i \\rangle = \\sum_{f=1}^k w_{uf} \\cdot h_{if}$$\n",
    "Besides the dot product ⟨⋅,⋅⟩, in general any kernel can be used.\n",
    "\n",
    "We can now specify the derivatives:\n",
    "$$ \\frac{\\partial}{\\partial \\theta} \\hat{x}_{uij} = \\begin{cases}\n",
    "(h_{if} - h_{jf}) \\text{ if } \\theta=w_{uf}, \\\\\n",
    "w_{uf} \\text{ if } \\theta = h_{if}, \\\\\n",
    "-w_{uf} \\text{ if } \\theta = h_{jf}, \\\\\n",
    "0 \\text{ else }\n",
    "\\end{cases} $$\n",
    "\n",
    "Which basically means: user $u$ prefer $i$ over $j$, let's do the following:\n",
    "- Increase the relevance (according to $u$) of features belonging to $i$ but not to $j$ and vice-versa\n",
    "- Increase the relevance of features assigned to $i$\n",
    "- Decrease the relevance of features assigned to $j$\n",
    "\n",
    "We're now ready to look at some code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Created on 07/09/17\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "from Recommender import Recommender\n",
    "import subprocess\n",
    "import os, sys\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class MF_BPR_Cython(Recommender):\n",
    "\n",
    "\n",
    "    def __init__(self, URM_train, positive_threshold=4, recompile_cython = False,\n",
    "                 num_factors=10):\n",
    "\n",
    "\n",
    "        super(MF_BPR_Cython, self).__init__()\n",
    "\n",
    "\n",
    "        self.URM_train = URM_train\n",
    "        self.n_users = URM_train.shape[0]\n",
    "        self.n_items = URM_train.shape[1]\n",
    "        self.normalize = False\n",
    "        self.num_factors = num_factors\n",
    "        self.positive_threshold = positive_threshold\n",
    "\n",
    "        if recompile_cython:\n",
    "            print(\"Compiling in Cython\")\n",
    "            self.runCompilationScript()\n",
    "            print(\"Compilation Complete\")\n",
    "\n",
    "\n",
    "\n",
    "    def fit(self, epochs=30, logFile=None, URM_test=None, filterTopPop = False, filterCustomItems = np.array([], dtype=np.int), minRatingsPerUser=1,\n",
    "            batch_size = 1000, validate_every_N_epochs = 1, start_validation_after_N_epochs = 0,\n",
    "            learning_rate = 0.05, sgd_mode='sgd', user_reg = 0.0, positive_reg = 0.0, negative_reg = 0.0):\n",
    "\n",
    "\n",
    "        self.eligibleUsers = []\n",
    "\n",
    "        # Select only positive interactions\n",
    "        URM_train_positive = self.URM_train.copy()\n",
    "\n",
    "        URM_train_positive.data = URM_train_positive.data >= self.positive_threshold\n",
    "        URM_train_positive.eliminate_zeros()\n",
    "\n",
    "\n",
    "        for user_id in range(self.n_users):\n",
    "\n",
    "            start_pos = URM_train_positive.indptr[user_id]\n",
    "            end_pos = URM_train_positive.indptr[user_id+1]\n",
    "\n",
    "            numUserInteractions = len(URM_train_positive.indices[start_pos:end_pos])\n",
    "\n",
    "            if  numUserInteractions > 0 and numUserInteractions<self.n_items:\n",
    "                self.eligibleUsers.append(user_id)\n",
    "\n",
    "        # self.eligibleUsers contains the userID having at least one positive interaction and one item non observed\n",
    "        self.eligibleUsers = np.array(self.eligibleUsers, dtype=np.int64)\n",
    "        self.sgd_mode = sgd_mode\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Import compiled module\n",
    "        from MatrixFactorization.Cython.MF_BPR_Cython_Epoch import MF_BPR_Cython_Epoch\n",
    "\n",
    "\n",
    "        self.cythonEpoch = MF_BPR_Cython_Epoch(URM_train_positive,\n",
    "                                                 self.eligibleUsers,\n",
    "                                                 num_factors = self.num_factors,\n",
    "                                                 learning_rate=learning_rate,\n",
    "                                                 batch_size=1,\n",
    "                                                 sgd_mode = sgd_mode,\n",
    "                                                 user_reg=user_reg,\n",
    "                                                 positive_reg=positive_reg,\n",
    "                                                 negative_reg=negative_reg)\n",
    "\n",
    "\n",
    "        self.batch_size = batch_size\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "\n",
    "        start_time_train = time.time()\n",
    "\n",
    "        for currentEpoch in range(epochs):\n",
    "\n",
    "            start_time_epoch = time.time()\n",
    "\n",
    "            if currentEpoch > 0:\n",
    "                if self.batch_size>0:\n",
    "                    self.epochIteration()\n",
    "                else:\n",
    "                    print(\"No batch not available\")\n",
    "\n",
    "\n",
    "            if (URM_test is not None) and (currentEpoch % validate_every_N_epochs == 0) and \\\n",
    "                            currentEpoch >= start_validation_after_N_epochs:\n",
    "\n",
    "                print(\"Evaluation begins\")\n",
    "\n",
    "                self.W = self.cythonEpoch.get_W()\n",
    "                self.H = self.cythonEpoch.get_H()\n",
    "\n",
    "                results_run = self.evaluateRecommendations(URM_test,\n",
    "                                                           minRatingsPerUser=minRatingsPerUser)\n",
    "\n",
    "                self.writeCurrentConfig(currentEpoch, results_run, logFile)\n",
    "\n",
    "                print(\"Epoch {} of {} complete in {:.2f} minutes\".format(currentEpoch, epochs,\n",
    "                                                                     float(time.time() - start_time_epoch) / 60))\n",
    "\n",
    "\n",
    "            # Fit with no validation\n",
    "            else:\n",
    "                print(\"Epoch {} of {} complete in {:.2f} minutes\".format(currentEpoch, epochs,\n",
    "                                                                         float(time.time() - start_time_epoch) / 60))\n",
    "\n",
    "        # Ensure W and H are up to date\n",
    "        self.W = self.cythonEpoch.get_W()\n",
    "        self.H = self.cythonEpoch.get_H()\n",
    "\n",
    "        print(\"Fit completed in {:.2f} minutes\".format(float(time.time() - start_time_train) / 60))\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def runCompilationScript(self):\n",
    "\n",
    "        # Run compile script setting the working directory to ensure the compiled file are contained in the\n",
    "        # appropriate subfolder and not the project root\n",
    "\n",
    "        compiledModuleSubfolder = \"/MatrixFactorization/Cython\"\n",
    "        fileToCompile_list = ['MF_BPR_Cython_Epoch.pyx']\n",
    "\n",
    "        for fileToCompile in fileToCompile_list:\n",
    "\n",
    "            command = ['python',\n",
    "                       'compileCython.py',\n",
    "                       fileToCompile,\n",
    "                       'build_ext',\n",
    "                       '--inplace'\n",
    "                       ]\n",
    "\n",
    "\n",
    "            output = subprocess.check_output(' '.join(command), shell=True, cwd=os.getcwd() + compiledModuleSubfolder)\n",
    "\n",
    "            try:\n",
    "\n",
    "                command = ['cython',\n",
    "                           fileToCompile,\n",
    "                           '-a'\n",
    "                           ]\n",
    "\n",
    "                output = subprocess.check_output(' '.join(command), shell=True, cwd=os.getcwd() + compiledModuleSubfolder)\n",
    "\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "\n",
    "        print(\"Compiled module saved in subfolder: {}\".format(compiledModuleSubfolder))\n",
    "\n",
    "        # Command to run compilation script\n",
    "        #python compileCython.py MF_BPR_Cython_Epoch.pyx build_ext --inplace\n",
    "\n",
    "        # Command to generate html report\n",
    "        #subprocess.call([\"cython\", \"-a\", \"MF_BPR_Cython_Epoch.pyx\"])\n",
    "\n",
    "\n",
    "    def epochIteration(self):\n",
    "\n",
    "        self.cythonEpoch.epochIteration_Cython()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def writeCurrentConfig(self, currentEpoch, results_run, logFile):\n",
    "\n",
    "        current_config = {'learn_rate': self.learning_rate,\n",
    "                          'num_factors': self.num_factors,\n",
    "                          'batch_size': 1,\n",
    "                          'epoch': currentEpoch}\n",
    "\n",
    "        print(\"Test case: {}\\nResults {}\\n\".format(current_config, results_run))\n",
    "\n",
    "        sys.stdout.flush()\n",
    "\n",
    "        if (logFile != None):\n",
    "            logFile.write(\"Test case: {}, Results {}\\n\".format(current_config, results_run))\n",
    "            logFile.flush()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def recommend(self, user_id, n=None, exclude_seen=True, filterTopPop = False, filterCustomItems = False):\n",
    "\n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM_train[user_id]\n",
    "\n",
    "        scores_array = np.dot(self.W[user_id], self.H.T)\n",
    "\n",
    "\n",
    "        if self.normalize:\n",
    "            # normalization will keep the scores in the same range\n",
    "            # of value of the ratings in dataset\n",
    "            rated = user_profile.copy()\n",
    "            rated.data = np.ones_like(rated.data)\n",
    "            if self.sparse_weights:\n",
    "                den = rated.dot(self.W_sparse).toarray().ravel()\n",
    "            else:\n",
    "                den = rated.dot(self.W).ravel()\n",
    "            den[np.abs(den) < 1e-6] = 1.0  # to avoid NaNs\n",
    "            scores_array /= den\n",
    "\n",
    "        if exclude_seen:\n",
    "            scores_array = self._filter_seen_on_scores(user_id, scores_array)\n",
    "\n",
    "        if filterTopPop:\n",
    "            scores_array = self._filter_TopPop_on_scores(scores_array)\n",
    "\n",
    "        if filterCustomItems:\n",
    "            scores_array = self._filterCustomItems_on_scores(scores_array)\n",
    "\n",
    "\n",
    "        # rank items and mirror column to obtain a ranking in descending score\n",
    "        #ranking = scores.argsort()\n",
    "        #ranking = np.flip(ranking, axis=0)\n",
    "\n",
    "        # Sorting is done in three steps. Faster then plain np.argsort for higher number of items\n",
    "        # - Partition the data to extract the set of relevant items\n",
    "        # - Sort only the relevant items\n",
    "        # - Get the original item index\n",
    "        relevant_items_partition = (-scores_array).argpartition(n)[0:n]\n",
    "        relevant_items_partition_sorting = np.argsort(-scores_array[relevant_items_partition])\n",
    "        ranking = relevant_items_partition[relevant_items_partition_sorting]\n",
    "\n",
    "\n",
    "        return ranking"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "And the cython code for the update\n",
    "\n",
    "\"\"\"\n",
    "Created on 07/09/17\n",
    "@author: Maurizio Ferrari Dacrema\n",
    "\"\"\"\n",
    "\n",
    "#cython: boundscheck=False\n",
    "#cython: wraparound=False\n",
    "#cython: initializedcheck=False\n",
    "#cython: language_level=3\n",
    "#cython: nonecheck=False\n",
    "#cython: cdivision=True\n",
    "#cython: unpack_method_calls=True\n",
    "#cython: overflowcheck=False\n",
    "\n",
    "#defining NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\n",
    "\n",
    "from Recommender_utils import check_matrix\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "import time\n",
    "import sys\n",
    "\n",
    "from libc.math cimport exp, sqrt\n",
    "from libc.stdlib cimport rand, RAND_MAX\n",
    "\n",
    "\n",
    "cdef struct BPR_sample:\n",
    "    long user\n",
    "    long pos_item\n",
    "    long neg_item\n",
    "\n",
    "\n",
    "cdef class MF_BPR_Cython_Epoch:\n",
    "\n",
    "    cdef int n_users\n",
    "    cdef int n_items, num_factors\n",
    "    cdef int numPositiveIteractions\n",
    "\n",
    "    cdef int useAdaGrad, rmsprop\n",
    "\n",
    "    cdef float learning_rate, user_reg, positive_reg, negative_reg\n",
    "\n",
    "    cdef int batch_size, sparse_weights\n",
    "\n",
    "    cdef long[:] eligibleUsers\n",
    "    cdef long numEligibleUsers\n",
    "\n",
    "    cdef int[:] seenItemsSampledUser\n",
    "    cdef int numSeenItemsSampledUser\n",
    "\n",
    "    cdef int[:] URM_mask_indices, URM_mask_indptr\n",
    "\n",
    "    cdef double[:,:] W, H\n",
    "\n",
    "\n",
    "    def __init__(self, URM_mask, eligibleUsers, num_factors,\n",
    "                 learning_rate = 0.05, user_reg = 0.0, positive_reg = 0.0, negative_reg = 0.0,\n",
    "                 batch_size = 1, sgd_mode='sgd'):\n",
    "\n",
    "        super(MF_BPR_Cython_Epoch, self).__init__()\n",
    "\n",
    "\n",
    "        URM_mask = check_matrix(URM_mask, 'csr')\n",
    "\n",
    "        self.numPositiveIteractions = int(URM_mask.nnz * 1)\n",
    "        self.n_users = URM_mask.shape[0]\n",
    "        self.n_items = URM_mask.shape[1]\n",
    "        self.num_factors = num_factors\n",
    "\n",
    "        self.URM_mask_indices = URM_mask.indices\n",
    "        self.URM_mask_indptr = URM_mask.indptr\n",
    "\n",
    "        # W and H cannot be initialized as zero, otherwise the gradient will always be zero\n",
    "        self.W = np.random.random((self.n_users, self.num_factors))\n",
    "        self.H = np.random.random((self.n_items, self.num_factors))\n",
    "\n",
    "\n",
    "\n",
    "        if sgd_mode=='sgd':\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"SGD_mode not valid. Acceptable values are: 'sgd'. Provided value was '{}'\".format(\n",
    "                    sgd_mode))\n",
    "\n",
    "\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.user_reg = user_reg\n",
    "        self.positive_reg = positive_reg\n",
    "        self.negative_reg = negative_reg\n",
    "\n",
    "\n",
    "        if batch_size!=1:\n",
    "            print(\"MiniBatch not implemented, reverting to default value 1\")\n",
    "        self.batch_size = 1\n",
    "\n",
    "        self.eligibleUsers = eligibleUsers\n",
    "        self.numEligibleUsers = len(eligibleUsers)\n",
    "\n",
    "\n",
    "    # Using memoryview instead of the sparse matrix itself allows for much faster access\n",
    "    cdef int[:] getSeenItems(self, long index):\n",
    "        return self.URM_mask_indices[self.URM_mask_indptr[index]:self.URM_mask_indptr[index + 1]]\n",
    "\n",
    "\n",
    "\n",
    "    def epochIteration_Cython(self):\n",
    "\n",
    "        # Get number of available interactions\n",
    "        cdef long totalNumberOfBatch = int(self.numPositiveIteractions / self.batch_size) + 1\n",
    "\n",
    "\n",
    "        cdef BPR_sample sample\n",
    "        cdef long u, i, j\n",
    "        cdef long index, numCurrentBatch\n",
    "        cdef double x_uij, sigmoid\n",
    "\n",
    "        cdef int numSeenItems\n",
    "\n",
    "        # Variables for AdaGrad and RMSprop\n",
    "        cdef double [:] sgd_cache\n",
    "        cdef double cacheUpdate\n",
    "        cdef float gamma\n",
    "\n",
    "        cdef double H_i, H_j, W_u\n",
    "\n",
    "        #\n",
    "        # if self.useAdaGrad:\n",
    "        #     sgd_cache = np.zeros((self.n_items), dtype=float)\n",
    "        #\n",
    "        # elif self.rmsprop:\n",
    "        #     sgd_cache = np.zeros((self.n_items), dtype=float)\n",
    "        #     gamma = 0.001\n",
    "\n",
    "\n",
    "        cdef long start_time_epoch = time.time()\n",
    "        cdef long start_time_batch = time.time()\n",
    "\n",
    "        for numCurrentBatch in range(totalNumberOfBatch):\n",
    "\n",
    "            # Uniform user sampling with replacement\n",
    "            sample = self.sampleBatch_Cython()\n",
    "\n",
    "            u = sample.user\n",
    "            i = sample.pos_item\n",
    "            j = sample.neg_item\n",
    "\n",
    "            x_uij = 0.0\n",
    "\n",
    "            for index in range(self.num_factors):\n",
    "\n",
    "                x_uij += self.W[u,index] * (self.H[i,index] - self.H[j,index])\n",
    "\n",
    "            # Use gradient of log(sigm(-x_uij))\n",
    "            sigmoid = 1 / (1 + exp(x_uij))\n",
    "\n",
    "\n",
    "            #   OLD CODE, YOU MAY TRY TO USE IT\n",
    "            #\n",
    "            # if self.useAdaGrad:\n",
    "            #     cacheUpdate = gradient ** 2\n",
    "            #\n",
    "            #     sgd_cache[i] += cacheUpdate\n",
    "            #     sgd_cache[j] += cacheUpdate\n",
    "            #\n",
    "            #     gradient = gradient / (sqrt(sgd_cache[i]) + 1e-8)\n",
    "            #\n",
    "            # elif self.rmsprop:\n",
    "            #     cacheUpdate = sgd_cache[i] * gamma + (1 - gamma) * gradient ** 2\n",
    "            #\n",
    "            #     sgd_cache[i] += cacheUpdate\n",
    "            #     sgd_cache[j] += cacheUpdate\n",
    "            #\n",
    "            #     gradient = gradient / (sqrt(sgd_cache[i]) + 1e-8)\n",
    "\n",
    "\n",
    "            for index in range(self.num_factors):\n",
    "\n",
    "                # Copy original value to avoid messing up the updates\n",
    "                H_i = self.H[i, index]\n",
    "                H_j = self.H[j, index]\n",
    "                W_u = self.W[u, index]\n",
    "\n",
    "                self.W[u, index] += self.learning_rate * (sigmoid * ( H_i - H_j ) - self.user_reg * W_u)\n",
    "                self.H[i, index] += self.learning_rate * (sigmoid * ( W_u ) - self.positive_reg * H_i)\n",
    "                self.H[j, index] += self.learning_rate * (sigmoid * (-W_u ) - self.negative_reg * H_j)\n",
    "\n",
    "\n",
    "\n",
    "            if((numCurrentBatch%5000000==0 and not numCurrentBatch==0) or numCurrentBatch==totalNumberOfBatch-1):\n",
    "                print(\"Processed {} ( {:.2f}% ) in {:.2f} seconds. Sample per second: {:.0f}\".format(\n",
    "                    numCurrentBatch*self.batch_size,\n",
    "                    100.0* float(numCurrentBatch*self.batch_size)/self.numPositiveIteractions,\n",
    "                    time.time() - start_time_batch,\n",
    "                    float(numCurrentBatch*self.batch_size + 1) / (time.time() - start_time_epoch)))\n",
    "\n",
    "                sys.stdout.flush()\n",
    "                sys.stderr.flush()\n",
    "\n",
    "                start_time_batch = time.time()\n",
    "\n",
    "\n",
    "    def get_W(self):\n",
    "\n",
    "        return np.array(self.W)\n",
    "\n",
    "\n",
    "    def get_H(self):\n",
    "        return np.array(self.H)\n",
    "\n",
    "\n",
    "\n",
    "    cdef BPR_sample sampleBatch_Cython(self):\n",
    "\n",
    "        cdef BPR_sample sample = BPR_sample()\n",
    "        cdef long index\n",
    "        cdef int negItemSelected\n",
    "\n",
    "        # Warning: rand() returns an integer\n",
    "\n",
    "        index = rand() % self.numEligibleUsers\n",
    "\n",
    "        sample.user = self.eligibleUsers[index]\n",
    "\n",
    "        self.seenItemsSampledUser = self.getSeenItems(sample.user)\n",
    "        self.numSeenItemsSampledUser = len(self.seenItemsSampledUser)\n",
    "\n",
    "        index = rand() % self.numSeenItemsSampledUser\n",
    "\n",
    "        sample.pos_item = self.seenItemsSampledUser[index]\n",
    "\n",
    "\n",
    "        negItemSelected = False\n",
    "\n",
    "        # It's faster to just try again then to build a mapping of the non-seen items\n",
    "        # for every user\n",
    "        while (not negItemSelected):\n",
    "            sample.neg_item = rand() % self.n_items\n",
    "\n",
    "            index = 0\n",
    "            while index < self.numSeenItemsSampledUser and self.seenItemsSampledUser[index]!=sample.neg_item:\n",
    "                index+=1\n",
    "\n",
    "            if index == self.numSeenItemsSampledUser:\n",
    "                negItemSelected = True\n",
    "\n",
    "return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movielens10MReader: loading data...\n",
      "Evaluation begins\n",
      "Processed 10000 ( 14.33% ) in 39.24 seconds. Users per second: 255\n",
      "Processed 20000 ( 28.66% ) in 76.87 seconds. Users per second: 260\n",
      "Processed 30000 ( 42.99% ) in 115.36 seconds. Users per second: 260\n",
      "Processed 40000 ( 57.32% ) in 152.21 seconds. Users per second: 263\n",
      "Processed 50000 ( 71.65% ) in 188.11 seconds. Users per second: 266\n",
      "Processed 60000 ( 85.98% ) in 223.98 seconds. Users per second: 268\n",
      "Test case: {'num_factors': 10, 'batch_size': 1, 'learn_rate': 0.0001, 'epoch': 0}\n",
      "Results {'MRR': 0.0013616214305503819, 'map': 0.00027624522719962749, 'precision': 0.00067352612421541554, 'NDCG': 0.00016576901095037021, 'AUC': 0.0014939382648820614, 'recall': 0.00016875698666090678}\n",
      "\n",
      "Epoch 0 of 5 complete in 4.32 minutes\n",
      "Processed 4005948 ( 100.00% ) in 6.18 seconds. Sample per second: 648136\n",
      "Epoch 1 of 5 complete in 0.10 minutes\n",
      "Processed 4005948 ( 100.00% ) in 6.31 seconds. Sample per second: 635091\n",
      "Evaluation begins\n",
      "Processed 10000 ( 14.33% ) in 36.43 seconds. Users per second: 274\n",
      "Processed 20000 ( 28.66% ) in 72.95 seconds. Users per second: 274\n",
      "Processed 30000 ( 42.99% ) in 109.52 seconds. Users per second: 274\n",
      "Processed 40000 ( 57.32% ) in 146.52 seconds. Users per second: 273\n",
      "Processed 50000 ( 71.65% ) in 183.67 seconds. Users per second: 272\n",
      "Processed 60000 ( 85.98% ) in 220.60 seconds. Users per second: 272\n",
      "Test case: {'num_factors': 10, 'batch_size': 1, 'learn_rate': 0.0001, 'epoch': 2}\n",
      "Results {'MRR': 0.31461671109073647, 'map': 0.12221666921429271, 'precision': 0.17518557794275666, 'NDCG': 0.078206134632386806, 'AUC': 0.28488602600479784, 'recall': 0.046078888098702191}\n",
      "\n",
      "Epoch 2 of 5 complete in 4.37 minutes\n",
      "Processed 4005948 ( 100.00% ) in 6.84 seconds. Sample per second: 585601\n",
      "Epoch 3 of 5 complete in 0.10 minutes\n",
      "Processed 4005948 ( 100.00% ) in 6.96 seconds. Sample per second: 575860\n",
      "Evaluation begins\n",
      "Processed 10000 ( 14.33% ) in 36.69 seconds. Users per second: 273\n",
      "Processed 20000 ( 28.66% ) in 73.38 seconds. Users per second: 273\n",
      "Processed 30000 ( 42.99% ) in 111.19 seconds. Users per second: 270\n",
      "Processed 40000 ( 57.32% ) in 148.27 seconds. Users per second: 270\n",
      "Processed 50000 ( 71.65% ) in 185.17 seconds. Users per second: 270\n",
      "Processed 60000 ( 85.98% ) in 221.73 seconds. Users per second: 271\n",
      "Test case: {'num_factors': 10, 'batch_size': 1, 'learn_rate': 0.0001, 'epoch': 4}\n",
      "Results {'MRR': 0.31820216292645309, 'map': 0.12595502581055212, 'precision': 0.17916081510993681, 'NDCG': 0.080135006471507286, 'AUC': 0.2866689595215583, 'recall': 0.047218963706113386}\n",
      "\n",
      "Epoch 4 of 5 complete in 4.40 minutes\n",
      "Fit completed in 13.29 minutes\n"
     ]
    }
   ],
   "source": [
    "from Movielens10MReader import Movielens10MReader\n",
    "\n",
    "dataReader = Movielens10MReader()\n",
    "\n",
    "URM_train = dataReader.get_URM_train()\n",
    "URM_test = dataReader.get_URM_test()\n",
    "\n",
    "recommender = MF_BPR_Cython(URM_train, recompile_cython=False, positive_threshold=4)\n",
    "\n",
    "logFile = open(\"Result_log.txt\", \"a\")\n",
    "\n",
    "recommender.fit(epochs=5, validate_every_N_epochs=2, URM_test=URM_test,\n",
    "                logFile=logFile, batch_size=1, sgd_mode='sgd', learning_rate=1e-4)\n",
    "\n",
    "#results_run = recommender.evaluateRecommendations(URM_test, at=5)\n",
    "#print(results_run)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
