{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems 2017/18\n",
    "\n",
    "### Practice 5 - SLIM BPR\n",
    "\n",
    "\n",
    "### State of the art machine learning algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A few info about gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from scipy import stats\n",
    "from scipy.optimize import fmin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Gradient descent</b>, also known as <b>steepest descent</b>, is an optimization algorithm for finding the local minimum of a function. To find a local minimum, the function \"steps\" in the  direction of the negative of the gradient. <b>Gradient ascent</b> is the same as gradient descent, except that it steps in the direction of the positive of the gradient and therefore finds local maximums instead of minimums. The algorithm of gradient descent can be outlined as follows:\n",
    "\n",
    "&nbsp;&nbsp;&nbsp; 1: &nbsp; Choose initial guess $x_0$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    2: &nbsp; <b>for</b> k = 0, 1, 2, ... <b>do</b> <br>\n",
    "&nbsp;&nbsp;&nbsp;    3:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $s_k$ = -$\\nabla f(x_k)$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    4:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; choose $\\alpha_k$ to minimize $f(x_k+\\alpha_k s_k)$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    5:   &nbsp;&nbsp;&nbsp;&nbsp;&nbsp; $x_{k+1} = x_k + \\alpha_k s_k$ <br>\n",
    "&nbsp;&nbsp;&nbsp;    6: &nbsp;  <b>end for</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a simple example, let's find a local minimum for the function $f(x) = x^3-2x^2+2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f = lambda x: x**3-2*x**2+2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXwAAAD8CAYAAAB0IB+mAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n0MJXSikf8f8BYcTkFoTygSUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac4773e710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = np.linspace(-1,2.5,1000)\n",
    "plt.plot(x,f(x))\n",
    "plt.xlim([-1,2.5])\n",
    "plt.ylim([0,3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from plot above that our local minimum is gonna be near around 1.4 or 1.5 (on the x-axis), but let's pretend that we don't know that, so we set our starting point (arbitrarily, in this case) at $x_0 = 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local minimum occurs at: 1.33\n",
      "Number of steps: 17\n"
     ]
    }
   ],
   "source": [
    "x_old = 0\n",
    "x_new = 2 # The algorithm starts at x=2\n",
    "n_k = 0.1 # step size\n",
    "precision = 0.0001\n",
    "\n",
    "x_list, y_list = [x_new], [f(x_new)]\n",
    "\n",
    "# returns the value of the derivative of our function\n",
    "def f_gradient(x):\n",
    "    return 3*x**2-4*x\n",
    " \n",
    "while abs(x_new - x_old) > precision:\n",
    "    \n",
    "    x_old = x_new\n",
    "    \n",
    "    # Gradient descent step\n",
    "    s_k = -f_gradient(x_old)\n",
    "    \n",
    "    x_new = x_old + n_k * s_k\n",
    "    \n",
    "    x_list.append(x_new)\n",
    "    y_list.append(f(x_new))\n",
    "    \n",
    "print (\"Local minimum occurs at: {:.2f}\".format(x_new))\n",
    "print (\"Number of steps:\", len(x_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The figures below show the route that was taken to find the local minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlYAAADSCAYAAACIG474AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nc845FyX/Dw1+b4yqnsbXAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fac443ddba8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=[10,3])\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(x_list,y_list,c=\"r\")\n",
    "plt.plot(x_list,y_list,c=\"r\")\n",
    "plt.plot(x,f(x), c=\"b\")\n",
    "plt.xlim([-1,2.5])\n",
    "plt.ylim([0,3])\n",
    "plt.title(\"Gradient descent\")\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(x_list,y_list,c=\"r\")\n",
    "plt.plot(x_list,y_list,c=\"r\")\n",
    "plt.plot(x,f(x), c=\"b\")\n",
    "plt.xlim([1.2,2.1])\n",
    "plt.ylim([0,3])\n",
    "plt.title(\"Gradient descent (zoomed in)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's implement SLIM BPR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In order to implement a SLIM BPR we need to:\n",
    "#### Randomly sample the triplets (user, positive_item, negative_item)\n",
    "#### Compute the score of each triplet\n",
    "#### Update the similarity matrix\n",
    "\n",
    "#### To simplify, we use a smaller dataset. More info on how to handle a bigger one later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movielens10MReader: loading data...\n",
      "Processed 1000000 cells\n",
      "Processed 2000000 cells\n",
      "Processed 3000000 cells\n",
      "Processed 4000000 cells\n",
      "Processed 5000000 cells\n",
      "Processed 6000000 cells\n",
      "Processed 7000000 cells\n",
      "Processed 8000000 cells\n",
      "Processed 1000000 cells\n"
     ]
    }
   ],
   "source": [
    "from Movielens10MReader import Movielens10MReader\n",
    "\n",
    "dataReader = Movielens10MReader()\n",
    "\n",
    "URM_train = dataReader.get_URM_train()\n",
    "URM_test = dataReader.get_URM_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<71568x5000 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 6769488 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_train = URM_train[:,0:5000]\n",
    "URM_test = URM_test[:,0:5000]\n",
    "URM_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1 - Sampling\n",
    "\n",
    "#### Create a mask of positive interactions. How to build it depends on the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<71568x5000 sparse matrix of type '<class 'numpy.float32'>'\n",
       "\twith 3915004 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "URM_mask = URM_train.copy()\n",
    "URM_mask.data[URM_mask.data <= 3] = 0\n",
    "\n",
    "URM_mask.eliminate_zeros()\n",
    "URM_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_users = URM_mask.shape[0]\n",
    "n_items = URM_mask.shape[1]\n",
    "\n",
    "\n",
    "# Extract users having at least one interaction to choose from\n",
    "eligibleUsers = []\n",
    "\n",
    "for user_id in range(n_users):\n",
    "\n",
    "    start_pos = URM_mask.indptr[user_id]\n",
    "    end_pos = URM_mask.indptr[user_id+1]\n",
    "\n",
    "    if len(URM_mask.indices[start_pos:end_pos]) > 0:\n",
    "        eligibleUsers.append(user_id)\n",
    "                \n",
    "                \n",
    "\n",
    "def sampleTriplet():\n",
    "    \n",
    "    # By randomly selecting a user in this way we could end up \n",
    "    # with a user with no interactions\n",
    "    #user_id = np.random.randint(0, n_users)\n",
    "    \n",
    "    user_id = np.random.choice(eligibleUsers)\n",
    "    \n",
    "    # Get user seen items and choose one\n",
    "    userSeenItems = URM_mask[user_id,:].indices\n",
    "    pos_item_id = np.random.choice(userSeenItems)\n",
    "\n",
    "    negItemSelected = False\n",
    "\n",
    "    # It's faster to just try again then to build a mapping of the non-seen items\n",
    "    while (not negItemSelected):\n",
    "        neg_item_id = np.random.randint(0, n_items)\n",
    "\n",
    "        if (neg_item_id not in userSeenItems):\n",
    "            \n",
    "            negItemSelected = True\n",
    "\n",
    "    return user_id, pos_item_id, neg_item_id\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(67895, 1961, 2769)\n",
      "(54071, 1387, 1602)\n",
      "(29860, 913, 300)\n",
      "(28977, 2791, 4965)\n",
      "(8569, 4874, 4536)\n",
      "(37131, 2078, 3968)\n",
      "(40196, 1374, 841)\n",
      "(60671, 4329, 1484)\n",
      "(36762, 1208, 2879)\n",
      "(3141, 590, 1851)\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(sampleTriplet())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2 - Computing prediction\n",
    "\n",
    "#### The prediction depends on the model: SLIM, Matrix Factorization... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We have to initialize our model. In case of SLIM it works best to initialize S as zero, in case of MF you cannot because of how the gradient is computed and you have to initialize at random. Here we initialize SLIM at random just so that we have some numbers to show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#similarity_matrix = np.zeros((n_items,n_items))\n",
    "\n",
    "similarity_matrix = np.random.random((n_items,n_items))\n",
    "similarity_matrix[np.arange(n_items),np.arange(n_items)] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "user_id, positive_item_id, negative_item_id = sampleTriplet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4970"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "positive_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3830"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "negative_item_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   1,   10,   29,   34,   60,  110,  111,  150,  207,  223,  260,\n",
       "        266,  296,  317,  326,  333,  344,  410,  441,  455,  457,  471,\n",
       "        480,  507,  515,  520,  527,  529,  539,  553,  586,  588,  589,\n",
       "        590,  592,  593,  594,  595,  596,  616,  648,  661,  709,  733,\n",
       "        736,  741,  750,  778,  830,  858,  899,  902,  903,  904,  905,\n",
       "        908,  909,  910,  912,  913,  918,  922,  923,  926,  928,  942,\n",
       "        947,  948,  950,  951,  953,  954,  965,  969, 1010, 1012, 1013,\n",
       "       1014, 1017, 1022, 1023, 1024, 1025, 1027, 1028, 1029, 1030, 1031,\n",
       "       1033, 1035, 1036, 1060, 1073, 1079, 1080, 1081, 1084, 1089, 1097,\n",
       "       1099, 1104, 1127, 1136, 1147, 1161, 1178, 1198, 1199, 1200, 1203,\n",
       "       1204, 1206, 1207, 1208, 1209, 1210, 1212, 1213, 1214, 1217, 1219,\n",
       "       1221, 1225, 1226, 1228, 1234, 1235, 1237, 1241, 1242, 1245, 1246,\n",
       "       1247, 1248, 1250, 1252, 1253, 1254, 1256, 1259, 1260, 1262, 1265,\n",
       "       1266, 1269, 1270, 1276, 1278, 1282, 1284, 1288, 1291, 1292, 1293,\n",
       "       1302, 1344, 1345, 1356, 1374, 1376, 1380, 1387, 1393, 1394, 1396,\n",
       "       1419, 1485, 1517, 1580, 1617, 1639, 1641, 1673, 1682, 1693, 1694,\n",
       "       1704, 1713, 1876, 1883, 1912, 1927, 1934, 1935, 1945, 1947, 1949,\n",
       "       1953, 1954, 1982, 1997, 2010, 2018, 2019, 2021, 2022, 2028, 2048,\n",
       "       2057, 2059, 2064, 2066, 2067, 2076, 2078, 2080, 2081, 2085, 2089,\n",
       "       2102, 2137, 2143, 2176, 2182, 2183, 2193, 2194, 2202, 2203, 2208,\n",
       "       2210, 2268, 2291, 2294, 2300, 2324, 2352, 2355, 2366, 2399, 2453,\n",
       "       2490, 2502, 2529, 2571, 2641, 2683, 2687, 2700, 2716, 2728, 2731,\n",
       "       2732, 2761, 2795, 2797, 2871, 2890, 2916, 2920, 2925, 2927, 2935,\n",
       "       2936, 2937, 2944, 2947, 2959, 2987, 2997, 3000, 3030, 3033, 3034,\n",
       "       3039, 3052, 3083, 3088, 3095, 3114, 3134, 3148, 3159, 3175, 3270,\n",
       "       3275, 3300, 3307, 3311, 3341, 3361, 3362, 3365, 3388, 3424, 3435,\n",
       "       3438, 3461, 3462, 3469, 3471, 3481, 3489, 3499, 3504, 3527, 3578,\n",
       "       3629, 3632, 3654, 3668, 3673, 3676, 3683, 3735, 3751, 3801, 3809,\n",
       "       3819, 3873, 3897, 3927, 3928, 3949, 3994, 4002, 4008, 4011, 4020,\n",
       "       4022, 4027, 4034, 4103, 4178, 4214, 4292, 4306, 4308, 4321, 4327,\n",
       "       4359, 4361, 4370, 4405, 4406, 4427, 4519, 4776, 4865, 4881, 4886,\n",
       "       4896, 4921, 4970, 4973, 4975, 4979, 4993, 4999], dtype=int32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userSeenItems = URM_mask[user_id,:].indices\n",
    "userSeenItems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_i is 172.65, x_j is 171.00\n"
     ]
    }
   ],
   "source": [
    "x_i = similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "x_j = similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "\n",
    "print(\"x_i is {:.2f}, x_j is {:.2f}\".format(x_i, x_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3 - Computing gradient\n",
    "\n",
    "#### The gradient depends on the objective function: RMSE, BPR... "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6485292979350277"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_ij = x_i - x_j\n",
    "x_ij"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The original BPR paper uses the logarithm of the sigmoid of x_ij, whose derivative is the following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16130781824274987"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gradient = 1 / (1 + np.exp(x_ij))\n",
    "gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 - Update model\n",
    "\n",
    "#### How to update depends on the model itself, here we have just one paramether, the similarity matrix, so we perform just one update. In matrix factorization we have two.\n",
    "\n",
    "#### We need a learning rate, which influences how fast the model will change. Small ones lead to slower convergence but often higher results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "\n",
    "similarity_matrix[positive_item_id, userSeenItems] += learning_rate * gradient\n",
    "similarity_matrix[positive_item_id, positive_item_id] = 0\n",
    "\n",
    "similarity_matrix[negative_item_id, userSeenItems] -= learning_rate * gradient\n",
    "similarity_matrix[negative_item_id, negative_item_id] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Usually there is no relevant change in the scores over a single iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_i is 172.71, x_j is 170.95\n"
     ]
    }
   ],
   "source": [
    "x_i = similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "x_j = similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "\n",
    "print(\"x_i is {:.2f}, x_j is {:.2f}\".format(x_i, x_j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5 - Write the iterative epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epochIteration():\n",
    "\n",
    "    # Get number of available interactions\n",
    "    numPositiveIteractions = int(URM_mask.nnz*0.01)\n",
    "\n",
    "    start_time_epoch = time.time()\n",
    "    start_time_batch = time.time()\n",
    "\n",
    "    # Uniform user sampling without replacement\n",
    "    for num_sample in range(numPositiveIteractions):\n",
    "\n",
    "        # Sample\n",
    "        user_id, positive_item_id, negative_item_id = sampleTriplet()\n",
    "        \n",
    "        userSeenItems = URM_mask[user_id,:].indices\n",
    "        \n",
    "        # Prediction\n",
    "        x_i = similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "        x_j = similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "        \n",
    "        # Gradient\n",
    "        x_ij = x_i - x_j\n",
    "\n",
    "        gradient = 1 / (1 + np.exp(x_ij))\n",
    "        \n",
    "        # Update\n",
    "        similarity_matrix[positive_item_id, userSeenItems] += learning_rate * gradient\n",
    "        similarity_matrix[positive_item_id, positive_item_id] = 0\n",
    "\n",
    "        similarity_matrix[negative_item_id, userSeenItems] -= learning_rate * gradient\n",
    "        similarity_matrix[negative_item_id, negative_item_id] = 0\n",
    "        \n",
    "\n",
    "        if(time.time() - start_time_batch >= 30 or num_sample == numPositiveIteractions-1):\n",
    "            print(\"Processed {} ( {:.2f}% ) in {:.2f} seconds. Sample per second: {:.0f}\".format(\n",
    "                num_sample,\n",
    "                100.0* float(num_sample)/numPositiveIteractions,\n",
    "                time.time() - start_time_batch,\n",
    "                float(num_sample) / (time.time() - start_time_epoch)))\n",
    "\n",
    "\n",
    "            start_time_batch = time.time()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8128 ( 20.76% ) in 30.00 seconds. Sample per second: 271\n",
      "Processed 16427 ( 41.96% ) in 30.00 seconds. Sample per second: 274\n",
      "Processed 24802 ( 63.35% ) in 30.00 seconds. Sample per second: 276\n",
      "Processed 33156 ( 84.69% ) in 30.00 seconds. Sample per second: 276\n",
      "Processed 39149 ( 100.00% ) in 21.56 seconds. Sample per second: 277\n"
     ]
    }
   ],
   "source": [
    "epochIteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.sparse as sps\n",
    "\n",
    "def similarityMatrixTopK(item_weights, forceSparseOutput = True, k=100, verbose = False, inplace=True):\n",
    "    \"\"\"\n",
    "    The function selects the TopK most similar elements, column-wise\n",
    "\n",
    "    :param item_weights:\n",
    "    :param forceSparseOutput:\n",
    "    :param k:\n",
    "    :param verbose:\n",
    "    :param inplace: Default True, WARNING matrix will be modified\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    assert (item_weights.shape[0] == item_weights.shape[1]), \"selectTopK: ItemWeights is not a square matrix\"\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Generating topK matrix\")\n",
    "\n",
    "    nitems = item_weights.shape[1]\n",
    "    k = min(k, nitems)\n",
    "\n",
    "    # for each column, keep only the top-k scored items\n",
    "    sparse_weights = not isinstance(item_weights, np.ndarray)\n",
    "\n",
    "    if not sparse_weights:\n",
    "\n",
    "        idx_sorted = np.argsort(item_weights, axis=0)  # sort data inside each column\n",
    "\n",
    "        if inplace:\n",
    "            W = item_weights\n",
    "        else:\n",
    "            W = item_weights.copy()\n",
    "\n",
    "        # index of the items that don't belong to the top-k similar items of each column\n",
    "        not_top_k = idx_sorted[:-k, :]\n",
    "        # use numpy fancy indexing to zero-out the values in sim without using a for loop\n",
    "        W[not_top_k, np.arange(nitems)] = 0.0\n",
    "\n",
    "        if forceSparseOutput:\n",
    "            W_sparse = sps.csr_matrix(W, shape=(nitems, nitems))\n",
    "\n",
    "            if verbose:\n",
    "                print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "            return W_sparse\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Dense TopK matrix generated in {:.2f} seconds\".format(time.time()-start_time))\n",
    "\n",
    "        return W\n",
    "\n",
    "    else:\n",
    "        # iterate over each column and keep only the top-k similar items\n",
    "        data, rows_indices, cols_indptr = [], [], []\n",
    "\n",
    "        item_weights = check_matrix(item_weights, format='csc', dtype=np.float32)\n",
    "\n",
    "        for item_idx in range(nitems):\n",
    "\n",
    "            cols_indptr.append(len(data))\n",
    "\n",
    "            start_position = item_weights.indptr[item_idx]\n",
    "            end_position = item_weights.indptr[item_idx+1]\n",
    "\n",
    "            column_data = item_weights.data[start_position:end_position]\n",
    "            column_row_index = item_weights.indices[start_position:end_position]\n",
    "\n",
    "            idx_sorted = np.argsort(column_data)  # sort by column\n",
    "            top_k_idx = idx_sorted[-k:]\n",
    "\n",
    "            data.extend(column_data[top_k_idx])\n",
    "            rows_indices.extend(column_row_index[top_k_idx])\n",
    "\n",
    "\n",
    "        cols_indptr.append(len(data))\n",
    "\n",
    "        # During testing CSR is faster\n",
    "        W_sparse = sps.csc_matrix((data, rows_indices, cols_indptr), shape=(nitems, nitems), dtype=np.float32)\n",
    "        W_sparse = W_sparse.tocsr()\n",
    "\n",
    "        if verbose:\n",
    "            print(\"Sparse TopK matrix generated in {:.2f} seconds\".format(time.time() - start_time))\n",
    "\n",
    "        return W_sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class SLIM_BPR_Recommender(object):\n",
    "    \"\"\" SLIM_BPR recommender with cosine similarity and no shrinkage\"\"\"\n",
    "\n",
    "    def __init__(self, URM, learning_rate = 0.01, epochs = 10):\n",
    "        self.URM = URM\n",
    "        self.learning_rate = learning_rate\n",
    "        self.epochs = epochs\n",
    "        \n",
    "        self.similarity_matrix = np.zeros((n_items,n_items))\n",
    "        \n",
    "        self.URM_mask = self.URM.copy()\n",
    "        self.URM_mask.data[self.URM_mask.data <= 3] = 0\n",
    "        self.URM_mask.eliminate_zeros()\n",
    "        \n",
    "        self.n_users = URM_mask.shape[0]\n",
    "        self.n_items = URM_mask.shape[1]\n",
    "\n",
    "\n",
    "        # Extract users having at least one interaction to choose from\n",
    "        self.eligibleUsers = []\n",
    "\n",
    "        for user_id in range(n_users):\n",
    "\n",
    "            start_pos = self.URM_mask.indptr[user_id]\n",
    "            end_pos = self.URM_mask.indptr[user_id+1]\n",
    "\n",
    "            if len(self.URM_mask.indices[start_pos:end_pos]) > 0:\n",
    "                self.eligibleUsers.append(user_id)\n",
    "\n",
    "\n",
    "\n",
    "    def sampleTriplet(self):\n",
    "\n",
    "        # By randomly selecting a user in this way we could end up \n",
    "        # with a user with no interactions\n",
    "        #user_id = np.random.randint(0, n_users)\n",
    "\n",
    "        user_id = np.random.choice(self.eligibleUsers)\n",
    "\n",
    "        # Get user seen items and choose one\n",
    "        userSeenItems = URM_mask[user_id,:].indices\n",
    "        pos_item_id = np.random.choice(userSeenItems)\n",
    "\n",
    "        negItemSelected = False\n",
    "\n",
    "        # It's faster to just try again then to build a mapping of the non-seen items\n",
    "        while (not negItemSelected):\n",
    "            neg_item_id = np.random.randint(0, n_items)\n",
    "\n",
    "            if (neg_item_id not in userSeenItems):\n",
    "\n",
    "                negItemSelected = True\n",
    "\n",
    "        return user_id, pos_item_id, neg_item_id\n",
    "\n",
    "\n",
    "        \n",
    "    def epochIteration(self):\n",
    "\n",
    "        # Get number of available interactions\n",
    "        numPositiveIteractions = int(self.URM_mask.nnz*0.01)\n",
    "\n",
    "        start_time_epoch = time.time()\n",
    "        start_time_batch = time.time()\n",
    "\n",
    "        # Uniform user sampling without replacement\n",
    "        for num_sample in range(numPositiveIteractions):\n",
    "\n",
    "            # Sample\n",
    "            user_id, positive_item_id, negative_item_id = self.sampleTriplet()\n",
    "\n",
    "            userSeenItems = self.URM_mask[user_id,:].indices\n",
    "\n",
    "            # Prediction\n",
    "            x_i = self.similarity_matrix[positive_item_id, userSeenItems].sum()\n",
    "            x_j = self.similarity_matrix[negative_item_id, userSeenItems].sum()\n",
    "\n",
    "            # Gradient\n",
    "            x_ij = x_i - x_j\n",
    "\n",
    "            gradient = 1 / (1 + np.exp(x_ij))\n",
    "\n",
    "            # Update\n",
    "            self.similarity_matrix[positive_item_id, userSeenItems] += learning_rate * gradient\n",
    "            self.similarity_matrix[positive_item_id, positive_item_id] = 0\n",
    "\n",
    "            self.similarity_matrix[negative_item_id, userSeenItems] -= learning_rate * gradient\n",
    "            self.similarity_matrix[negative_item_id, negative_item_id] = 0\n",
    "\n",
    "\n",
    "            if(time.time() - start_time_batch >= 30 or num_sample == numPositiveIteractions-1):\n",
    "                print(\"Processed {} ( {:.2f}% ) in {:.2f} seconds. Sample per second: {:.0f}\".format(\n",
    "                    num_sample,\n",
    "                    100.0* float(num_sample)/numPositiveIteractions,\n",
    "                    time.time() - start_time_batch,\n",
    "                    float(num_sample) / (time.time() - start_time_epoch)))\n",
    "\n",
    "                start_time_batch = time.time()\n",
    "\n",
    "                \n",
    "    def fit(self):\n",
    "        \n",
    "        for numEpoch in range(self.epochs):\n",
    "            self.epochIteration()\n",
    "            \n",
    "        self.similarity_matrix = self.similarity_matrix.T\n",
    "        \n",
    "        self.similarity_matrix = similarityMatrixTopK(self.similarity_matrix, k=100)\n",
    "        \n",
    "        \n",
    "    def recommend(self, user_id, at=None, exclude_seen=True):\n",
    "        \n",
    "        # compute the scores using the dot product\n",
    "        user_profile = self.URM[user_id]\n",
    "        scores = user_profile.dot(self.similarity_matrix)\n",
    "        scores = scores.toarray()\n",
    "\n",
    "        # rank items\n",
    "        ranking = scores.argsort()[::-1].squeeze()\n",
    "        if exclude_seen:\n",
    "            ranking = self._filter_seen(user_id, ranking)\n",
    "            \n",
    "        return ranking[:at]\n",
    "    \n",
    "    def _filter_seen(self, user_id, ranking):\n",
    "        user_profile = self.URM[user_id]\n",
    "        seen = user_profile.indices\n",
    "        unseen_mask = np.in1d(ranking, seen, assume_unique=True, invert=True)\n",
    "        return ranking[unseen_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 8357 ( 21.35% ) in 30.00 seconds. Sample per second: 279\n",
      "Processed 16652 ( 42.53% ) in 30.00 seconds. Sample per second: 278\n",
      "Processed 25006 ( 63.87% ) in 30.00 seconds. Sample per second: 278\n",
      "Processed 33369 ( 85.23% ) in 30.00 seconds. Sample per second: 278\n",
      "Processed 39149 ( 100.00% ) in 20.71 seconds. Sample per second: 278\n"
     ]
    }
   ],
   "source": [
    "recommender = SLIM_BPR_Recommender(URM_train, epochs=1)\n",
    "recommender.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def precision(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    precision_score = np.sum(is_relevant, dtype=np.float32) / len(is_relevant)\n",
    "    \n",
    "    return precision_score\n",
    "\n",
    "def recall(recommended_items, relevant_items):\n",
    "    \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    recall_score = np.sum(is_relevant, dtype=np.float32) / relevant_items.shape[0]\n",
    "    \n",
    "    return recall_score\n",
    "\n",
    "def MAP(recommended_items, relevant_items):\n",
    "   \n",
    "    is_relevant = np.in1d(recommended_items, relevant_items, assume_unique=True)\n",
    "    \n",
    "    # Cumulative sum: precision at 1, at 2, at 3 ...\n",
    "    p_at_k = is_relevant * np.cumsum(is_relevant, dtype=np.float32) / (1 + np.arange(is_relevant.shape[0]))\n",
    "    \n",
    "    map_score = np.sum(p_at_k) / np.min([relevant_items.shape[0], is_relevant.shape[0]])\n",
    "\n",
    "    return map_score\n",
    "\n",
    "def evaluate_algorithm(URM_test, recommender_object, at=5):\n",
    "    \n",
    "    cumulative_precision = 0.0\n",
    "    cumulative_recall = 0.0\n",
    "    cumulative_MAP = 0.0\n",
    "    \n",
    "    num_eval = 0\n",
    "\n",
    "\n",
    "    for user_id in range(URM_test.shape[0]):\n",
    "        \n",
    "        if user_id % 10000 == 0:\n",
    "            print(\"Processed user: {} \".format(user_id))\n",
    "\n",
    "        relevant_items = URM_test[user_id].indices\n",
    "        \n",
    "        if len(relevant_items)>0:\n",
    "            \n",
    "            recommended_items = recommender_object.recommend(user_id, at=at)\n",
    "            num_eval+=1\n",
    "\n",
    "            cumulative_precision += precision(recommended_items, relevant_items)\n",
    "            cumulative_recall += recall(recommended_items, relevant_items)\n",
    "            cumulative_MAP += MAP(recommended_items, relevant_items)\n",
    "\n",
    "\n",
    "    cumulative_precision /= num_eval\n",
    "    cumulative_recall /= num_eval\n",
    "    cumulative_MAP /= num_eval\n",
    "    \n",
    "    print(\"Recommender performance is: Precision = {:.4f}, Recall = {:.4f}, MAP = {:.4f}\".format(\n",
    "        cumulative_precision, cumulative_recall, cumulative_MAP))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed user: 0 \n",
      "Processed user: 10000 \n",
      "Processed user: 20000 \n",
      "Processed user: 30000 \n",
      "Processed user: 40000 \n",
      "Processed user: 50000 \n",
      "Processed user: 60000 \n",
      "Processed user: 70000 \n",
      "Recommender performance is: Precision = 0.0007, Recall = 0.0001, MAP = 0.0002\n"
     ]
    }
   ],
   "source": [
    "evaluate_algorithm(URM_test, recommender)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Horrible performance!!!\n",
    "#### Of course, we initialized the model as zero and performed just one epoch on a micro-subsample of the data. Being a machine learning approach, more time is needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's load the cython impementation and run some serious learning. Here I show just two epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Movielens10MReader: loading data...\n",
      "Processed 1000000 cells\n",
      "Processed 2000000 cells\n",
      "Processed 3000000 cells\n",
      "Processed 4000000 cells\n",
      "Processed 5000000 cells\n",
      "Processed 6000000 cells\n",
      "Processed 7000000 cells\n",
      "Processed 8000000 cells\n",
      "Processed 1000000 cells\n",
      "Evaluation begins\n",
      "Processed 10000 ( 14.33% ) in 7.93 seconds. Users per second: 1261\n",
      "Processed 20000 ( 28.65% ) in 15.87 seconds. Users per second: 1260\n",
      "Processed 30000 ( 42.98% ) in 23.80 seconds. Users per second: 1261\n",
      "Processed 40000 ( 57.31% ) in 31.60 seconds. Users per second: 1266\n",
      "Processed 50000 ( 71.64% ) in 39.39 seconds. Users per second: 1269\n",
      "Processed 60000 ( 85.96% ) in 47.18 seconds. Users per second: 1272\n",
      "Test case: {'learn_rate': 0.0001, 'topK_similarity': False, 'epoch': 0, 'sgd_mode': 'sgd'}\n",
      "Results {'AUC': 3.9400538712820218e-05, 'precision': 2.5789443521118686e-05, 'recall': 1.5891262870766795e-06, 'map': 8.8830305461631038e-06, 'NDCG': 2.5000338141540997e-06, 'MRR': 4.4415152730815524e-05}\n",
      "\n",
      "Epoch 0 of 2 complete in 0.91 minutes\n",
      "Processed 500000 ( 12.49% ) in 31.60 seconds. Sample per second: 15822\n",
      "Processed 1000000 ( 24.98% ) in 34.93 seconds. Sample per second: 15167\n",
      "Processed 1500000 ( 37.47% ) in 36.48 seconds. Sample per second: 14781\n",
      "Processed 2000000 ( 49.95% ) in 36.73 seconds. Sample per second: 14521\n",
      "Processed 2500000 ( 62.44% ) in 37.53 seconds. Sample per second: 14324\n",
      "Processed 3000000 ( 74.93% ) in 37.90 seconds. Sample per second: 14158\n",
      "Processed 3500000 ( 87.42% ) in 38.40 seconds. Sample per second: 14034\n",
      "Processed 4000000 ( 99.91% ) in 38.18 seconds. Sample per second: 13928\n",
      "Processed 4003654 ( 100.00% ) in 0.46 seconds. Sample per second: 13928\n",
      "Return S matrix to python caller\n",
      "Evaluation begins\n",
      "Processed 10000 ( 14.33% ) in 25.60 seconds. Users per second: 391\n",
      "Processed 20000 ( 28.65% ) in 51.33 seconds. Users per second: 390\n",
      "Processed 30000 ( 42.98% ) in 77.16 seconds. Users per second: 389\n",
      "Processed 40000 ( 57.31% ) in 102.72 seconds. Users per second: 389\n",
      "Processed 50000 ( 71.64% ) in 128.51 seconds. Users per second: 389\n",
      "Processed 60000 ( 85.96% ) in 154.74 seconds. Users per second: 388\n",
      "Test case: {'learn_rate': 0.0001, 'topK_similarity': False, 'epoch': 1, 'sgd_mode': 'sgd'}\n",
      "Results {'AUC': 0.38039190402506207, 'precision': 0.26208092154284879, 'recall': 0.082374223953627215, 'map': 0.19660219703775766, 'NDCG': 0.13348504967222796, 'MRR': 0.44007751160523423}\n",
      "\n",
      "Epoch 1 of 2 complete in 8.30 minutes\n",
      "Fit completed in 9.21 minutes\n"
     ]
    }
   ],
   "source": [
    "from run_me import run_SLIM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What you see is:\n",
    "* The initial performance with the model initialized as zero\n",
    "* The performance after the first iteration\n",
    "\n",
    "### Compared to content and collaborative k-nearest neighbor SLIM improves map significantly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
